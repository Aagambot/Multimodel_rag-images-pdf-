{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.3.0 which is incompatible.\n",
      "xformers 0.0.29.post3 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Installations\n",
    "%pip install -qU langchain langchain-google-genai langchain_community pymupdf pillow chromadb transformers torch sentence-transformers\n",
    "#restart the kernal after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Using cached arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\aagam\\anaconda3\\lib\\site-packages (from arxiv) (2.32.3)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aagam\\anaconda3\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aagam\\anaconda3\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aagam\\anaconda3\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aagam\\anaconda3\\lib\\site-packages (from requests~=2.32.0->arxiv) (2025.1.31)\n",
      "Using cached arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6060 sha256=851e99e613c1f39681f9e783ebd14c91d5e7724d4389541373d05e181a3e22ef\n",
      "  Stored in directory: c:\\users\\aagam\\appdata\\local\\pip\\cache\\wheels\\03\\f5\\1a\\23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API Key is already set.\n"
     ]
    }
   ],
   "source": [
    "# utils.py\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import arxiv\n",
    "import uuid\n",
    "\n",
    "# LangChain Imports\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# --- API Key Setup ---\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Provide your Google API Key: \")\n",
    "    print(\"✅ API Key has been set for this session.\")\n",
    "else:\n",
    "    print(\"✅ API Key is already set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import arxiv\n",
    "import os\n",
    "\n",
    "def find_and_download_paper(query: str, min_pages: int = 15, max_pages: int = 25, download_path: str = \".\"):\n",
    "    \"\"\"\n",
    "    Searches ArXiv for papers on a topic and downloads the first result\n",
    "    that falls within the specified page range.\n",
    "    \"\"\"\n",
    "    print(f\"Searching ArXiv for papers on '{query}' between {min_pages}-{max_pages} pages...\")\n",
    "    try:\n",
    "        client = arxiv.Client(page_size=20, delay_seconds=3, num_retries=3)\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=20,\n",
    "            sort_by=arxiv.SortCriterion.Relevance,\n",
    "        )\n",
    "\n",
    "        # Using client.results(search) is the new, correct method\n",
    "        for result in client.results(search):\n",
    "            if result.comment and 'pages' in result.comment.lower():\n",
    "                match = re.search(r'(\\d+)\\s*pages', result.comment, re.IGNORECASE)\n",
    "                if match:\n",
    "                    pages = int(match.group(1))\n",
    "                    print(f\"  > Found '{result.title}' ({pages} pages)... checking range.\")\n",
    "                    \n",
    "                    if min_pages <= pages <= max_pages:\n",
    "                        print(f\"    > Match found! Attempting to download...\")\n",
    "                        try:\n",
    "                            filename = f\"{result.entry_id.split('/')[-1]}.pdf\"\n",
    "                            filepath = os.path.join(download_path, filename)\n",
    "                            result.download_pdf(dirpath=download_path, filename=filename)\n",
    "                            print(f\"Successfully downloaded '{result.title}'\")\n",
    "                            return {\"title\": result.title, \"filepath\": filepath}\n",
    "                        except Exception as e:\n",
    "                            print(f\"    > Download failed: {e}. Trying the next paper.\")\n",
    "                            continue\n",
    "\n",
    "        print(f\"Sorry, could not find a paper between {min_pages}-{max_pages} pages in the top 20 results.\")\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the ArXiv search: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Extract Elements from PDF ---\n",
    "def extract_pdf_elements(pdf_path):\n",
    "    \"\"\"Extracts text and images from a PDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_chunks, images = [], []\n",
    "    for page in doc:\n",
    "        text = page.get_text()\n",
    "        if text.strip():\n",
    "            text_chunks.append(text)\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image = Image.open(io.BytesIO(base_image[\"image\"]))\n",
    "            images.append(image)\n",
    "    return text_chunks, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Generate Image Summaries (Corrected Version) ---\n",
    "def generate_image_summaries(images):\n",
    "    \"\"\"\n",
    "    Generates text summaries for a list of images using Gemini.\n",
    "    This version uses the correct model name and includes a delay for rate limiting.\n",
    "    \"\"\"\n",
    "    import base64\n",
    "    import io\n",
    "    import time # <-- Add this import\n",
    "\n",
    "    # Use the correct, available model name\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\", \n",
    "        temperature=0,\n",
    "        google_api_key=os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    summaries = []\n",
    "    print(f\"Summarizing {len(images)} images (1 per second)...\")\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        buffered = io.BytesIO()\n",
    "        if img.mode == 'RGBA':\n",
    "            img = img.convert('RGB')\n",
    "        img.save(buffered, format=\"JPEG\")\n",
    "        img_base64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "        \n",
    "        image_content = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": f\"data:image/jpeg;base64,{img_base64}\",\n",
    "        }\n",
    "\n",
    "        prompt = [HumanMessage(content=[\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image from a research paper. What is it showing? Be detailed.\"},\n",
    "            image_content,\n",
    "        ])]\n",
    "        \n",
    "        response = model.invoke(prompt)\n",
    "        summaries.append(response.content)\n",
    "        print(f\"  > Summarized image {i + 1}/{len(images)}\")\n",
    "        \n",
    "        # Add a 1-second delay to stay within API rate limits\n",
    "        time.sleep(1)\n",
    "        \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Build the Multi-Vector Retriever ---\n",
    "def build_multimodal_retriever(pdf_path):\n",
    "    \"\"\"Builds the core RAG retriever from a PDF file.\"\"\"\n",
    "    id_key = \"doc_id\"\n",
    "    \n",
    "    # The vectorstore to use to index the child chunks\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"multimodal_rag\",\n",
    "        embedding_function=SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    )\n",
    "    # The storage layer for the parent documents\n",
    "    docstore = InMemoryStore()\n",
    "\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=docstore,\n",
    "        id_key=id_key,\n",
    "        search_kwargs={'k': 5}\n",
    "    )\n",
    "\n",
    "    # Extract elements\n",
    "    raw_text_chunks, raw_images = extract_pdf_elements(pdf_path)\n",
    "    \n",
    "    # Add text to retriever\n",
    "    doc_ids_text = [str(uuid.uuid4()) for _ in raw_text_chunks]\n",
    "    text_docs = [Document(page_content=chunk, metadata={id_key: doc_ids_text[i]}) for i, chunk in enumerate(raw_text_chunks)]\n",
    "    retriever.vectorstore.add_documents(text_docs)\n",
    "    retriever.docstore.mset(list(zip(doc_ids_text, raw_text_chunks)))\n",
    "\n",
    "    # Add images and their summaries to retriever\n",
    "    if raw_images:\n",
    "        image_summaries = generate_image_summaries(raw_images)\n",
    "        doc_ids_img = [str(uuid.uuid4()) for _ in raw_images]\n",
    "        summary_docs = [Document(page_content=summary, metadata={id_key: doc_ids_img[i]}) for i, summary in enumerate(image_summaries)]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids_img, raw_images)))\n",
    "    \n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook cell\n",
    "\n",
    "def create_rag_chain(retriever):\n",
    "    \"\"\"Creates the final RAG chain for querying.\"\"\"\n",
    "    import base64\n",
    "    import io\n",
    "\n",
    "    def format_context_for_gemini(docs):\n",
    "        \"\"\"\n",
    "        Formats retrieved documents (text and images) for the Gemini model.\n",
    "        Images are converted to Base64 data URIs.\n",
    "        \"\"\"\n",
    "        context_parts = []\n",
    "        for doc in docs:\n",
    "            # The MultiVectorRetriever returns the raw doc from the docstore.\n",
    "            # It can be a string (from text chunks) or a PIL Image object.\n",
    "            if isinstance(doc, str):\n",
    "                # --- THIS IS THE FIX ---\n",
    "                # Correctly append the text part\n",
    "                context_parts.append({\"type\": \"text\", \"text\": doc})\n",
    "            elif isinstance(doc, Image.Image):\n",
    "                # Convert PIL Image to Base64\n",
    "                buffered = io.BytesIO()\n",
    "                if doc.mode == 'RGBA':\n",
    "                    doc = doc.convert('RGB')\n",
    "                doc.save(buffered, format=\"JPEG\")\n",
    "                img_base64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "                \n",
    "                context_parts.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{img_base64}\",\n",
    "                })\n",
    "        return context_parts\n",
    "\n",
    "    def create_prompt(context_parts, question):\n",
    "        \"\"\"Creates a multimodal prompt from the context parts and question.\"\"\"\n",
    "        prompt_str = f\"\"\"You are an expert research assistant. Synthesize a comprehensive answer to the user's question using all the provided context below. The context contains both text excerpts and images from a research paper. You must use information from both the text and the images to form your answer.\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Context:\n",
    "        \"\"\"\n",
    "        final_prompt_content = [{\"type\": \"text\", \"text\": prompt_str}]\n",
    "        final_prompt_content.extend(context_parts)\n",
    "        \n",
    "        return [HumanMessage(content=final_prompt_content)]\n",
    "\n",
    "    # Initialize the final, powerful model\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=0.2,\n",
    "        google_api_key=os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    # Define the final chain\n",
    "    chain = (\n",
    "        {\"context\": retriever | RunnableLambda(format_context_for_gemini), \"question\": RunnablePassthrough()}\n",
    "        | RunnableLambda(lambda x: create_prompt(x[\"context\"], x[\"question\"]))\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching ArXiv for papers on 'multimodal large language models for robotics' between 15-25 pages...\n",
      "  > Found 'Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision' (27 pages)... checking range.\n",
      "  > Found 'Integrating Large Language Models with Multimodal Virtual Reality Interfaces to Support Collaborative Human-Robot Construction Work' (39 pages)... checking range.\n",
      "  > Found 'Multimodal Spatial Language Maps for Robot Navigation and Manipulation' (24 pages)... checking range.\n",
      "    > Match found! Attempting to download...\n",
      "Successfully downloaded 'Multimodal Spatial Language Maps for Robot Navigation and Manipulation'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aagam\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3452: DecompressionBombWarning: Image size (172120040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing 14 images (1 per second)...\n",
      "  > Summarized image 1/14\n",
      "  > Summarized image 2/14\n",
      "  > Summarized image 3/14\n",
      "  > Summarized image 4/14\n",
      "  > Summarized image 5/14\n",
      "  > Summarized image 6/14\n",
      "  > Summarized image 7/14\n",
      "  > Summarized image 8/14\n",
      "  > Summarized image 9/14\n",
      "  > Summarized image 10/14\n",
      "  > Summarized image 11/14\n",
      "  > Summarized image 12/14\n",
      "  > Summarized image 13/14\n",
      "  > Summarized image 14/14\n",
      "\n",
      "==================================================\n",
      "Asking question: According to the paper, what are the main challenges in applying multimodal models to robotics? Refer to any diagrams if possible.\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "According to the paper, the main challenges in applying multimodal models to robotics are:\n",
       "\n",
       "1.  **Data Collection and Annotation:** It is difficult to acquire and accurately label high-dimensional, synchronized, and diverse data from robotic interactions, as this data is dynamic and highly dependent on the robot's state and environment.\n",
       "2.  **Real-time Inference and Low-latency Control:** Complex multimodal models are computationally intensive, leading to high latency that is unacceptable for the immediate responses required for safe and effective real-time robotic control.\n",
       "3.  **Generalization and Robustness:** Models trained on limited datasets struggle to adapt and perform reliably in novel, unstructured real-world environments, or when encountering unseen conditions or variations.\n",
       "4.  **Interpretability and Explainability:** The black-box nature of many deep learning models makes it difficult to understand the reasoning behind a robot's decisions, which hinders debugging, diagnosis of failures, and building trust in autonomous systems.\n",
       "\n",
       "Figure 2, titled \"Challenges in Multimodal Robotics,\" explicitly illustrates these four points: \"Data Collection & Annotation,\" \"Real-time Inference & Control,\" \"Generalization & Robustness,\" and \"Interpretability & Explainability,\" along with brief descriptions for each."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Block 4: Define your topic and question, then run the entire process.\n",
    "\n",
    "# 1. SET YOUR RESEARCH TOPIC HERE\n",
    "topic = \"multimodal large language models for robotics\"\n",
    "\n",
    "# 2. Find and process the paper\n",
    "paper_info = find_and_download_paper(topic)\n",
    "\n",
    "if paper_info:\n",
    "    # 3. Build the RAG retriever for the downloaded paper\n",
    "    \n",
    "    retriever = build_multimodal_retriever(paper_info['filepath'])\n",
    "    # 4. Create the final query chain\n",
    "    rag_chain = create_rag_chain(retriever)\n",
    "    \n",
    "    # 5. SET YOU    R QUESTION ABOUT THE PAPER HERE\n",
    "    question = \"According to the paper, what are the main challenges in applying multimodal models to robotics? Refer to any diagrams if possible.\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Asking question: {question}\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    # 6. Invoke the chain and get the answer\n",
    "    response = rag_chain.invoke(question)\n",
    "    \n",
    "    # 7. Display the final answer\n",
    "    display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Asking question: what is the important point research paper is trying to tell \n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided images, the important point the research paper is trying to convey is the ability to perform **cross-modal reasoning** (combining visual/object information with auditory/sound information) within a 3D environment.\n",
       "\n",
       "This is demonstrated through:\n",
       "1.  **VLMap Creation and Landmark Indexing:** Building a 3D map that integrates visual information and can be semantically indexed with open-vocabulary text labels for both objects and sounds.\n",
       "2.  **Combined Queries:** The system can answer complex queries that relate objects to sounds (e.g., \"the backpack near the sound of glass breaking\" or \"the shelf near the sound of door knock\").\n",
       "3.  **Explicit Cross-Modal Reasoning:** The final image explicitly shows how \"Sound Prediction\" and \"Object Prediction\" are combined through \"Cross-Modal Reasoning\" to answer a query like \"the sound of baby crying near the sofa,\" indicating the system's capability to understand and locate entities based on their inter-modal spatial relationships."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = input(\"Enter your question: \")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Asking question: {question}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    # 6. Invoke the chain and get the answer\n",
    "response = rag_chain.invoke(question)\n",
    "    \n",
    "    # 7. Display the final answer\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
